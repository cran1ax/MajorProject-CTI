# ==================== COMPLETE CYBERSECURITY PROJECT (v3.0 - Revised) ====================
import pandas as pd
import numpy as np
import streamlit as st
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
import plotly.express as px
import joblib
import re
import urllib.parse as urlparse
import warnings
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model # Added load_model import
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

warnings.filterwarnings('ignore')

# Helper function for generating synthetic URL data
def generate_mock_url_data(n_samples=500):
    """Generates synthetic URLs and their binary risk labels."""
    safe_domains = ['google.com', 'microsoft.com', 'wikipedia.org', 'streamlit.io', 'github.com']
    mal_patterns = ['login', 'verify', 'update', 'docs', 'secure', 'bank']
    
    urls = []
    risks = []
    
    for i in range(n_samples):
        is_malicious = np.random.rand() < 0.3 # 30% malicious
        if is_malicious:
            # Generate malicious-looking URL
            domain = np.random.choice(safe_domains)
            path = np.random.choice(mal_patterns) + str(np.random.randint(100, 999)) + ".html"
            query_length = np.random.randint(5, 15)
            url = f"http://{domain}-{np.random.randint(10, 99)}/{path}?id={query_length}"
            if np.random.rand() < 0.4: url = url.replace('http', 'https')
            if np.random.rand() < 0.2: url = "http://192.168.1.1/" + path # Use IP
            if np.random.rand() < 0.2: url = url.replace('.', '..')
            risks.append(1)
        else:
            # Generate safe-looking URL
            domain = np.random.choice(safe_domains)
            path = np.random.choice(['/search', '/about', '/info', '/blog'])
            url = f"https://www.{domain}{path}/{np.random.randint(1000, 9999)}"
            risks.append(0)
        urls.append(url)
        
    return urls, np.array(risks)


# ==================== 1. DATA GENERATION (Incident Metrics) ====================
print("üîÑ Step 1: Generating Incident Data...")
def generate_cybersecurity_data(n_samples=2000):
    """Generates the main incident data with metrics and categories."""
    np.random.seed(42)
    attack_types = ['Phishing', 'Ransomware', 'DDoS', 'Malware', 'Insider Threat']
    industries = ['Finance', 'Healthcare', 'Government', 'Education', 'Technology']
    countries = ['USA', 'UK', 'Germany', 'India', 'Japan'] # Re-added Countries
    
    data = {
        'timestamp': pd.date_range('2020-01-01', periods=n_samples, freq='D'),
        'attack_type': np.random.choice(attack_types, n_samples),
        'target_industry': np.random.choice(industries, n_samples),
        'country': np.random.choice(countries, n_samples), # Include country
        'financial_loss': np.random.exponential(50, n_samples),
        'affected_users': np.random.poisson(10000, n_samples),
        'response_time': np.random.gamma(2, 2, n_samples),
        'data_breach_size': np.random.lognormal(8, 2, n_samples),
        'network_traffic': np.random.normal(1000, 300, n_samples),
        'vulnerability_score': np.random.uniform(1, 10, n_samples),
    }

    risk_conditions = (
        (data['financial_loss'] > 80) |
        (data['affected_users'] > 30000) |
        (data['response_time'] > 8)
    )
    data['high_risk_incident'] = risk_conditions.astype(int)
    df = pd.DataFrame(data)
    return df

df = generate_cybersecurity_data()


# ==================== 2. FEATURE ENGINEERING (Incident Metrics) ====================
print("üîç Step 2: Feature Engineering...")
df['year'] = df['timestamp'].dt.year
df['month'] = df['timestamp'].dt.month
df['loss_per_user'] = df['financial_loss'] / (df['affected_users'] + 1)

feature_columns = [
    'financial_loss', 'affected_users', 'response_time',
    'data_breach_size', 'network_traffic', 'vulnerability_score', 'loss_per_user'
]

le_attack = LabelEncoder()
le_industry = LabelEncoder()
le_country = LabelEncoder() # Added Country Encoder

df['attack_type_encoded'] = le_attack.fit_transform(df['attack_type'])
df['industry_encoded'] = le_industry.fit_transform(df['target_industry'])
df['country_encoded'] = le_country.fit_transform(df['country']) # Fit/Transform Country

feature_columns.extend(['attack_type_encoded', 'industry_encoded', 'country_encoded', 'year', 'month']) # Added country_encoded
X = df[feature_columns]
y = df['high_risk_incident']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)


# ==================== 3. MAIN INCIDENT MODEL TRAINING ====================
print("ü§ñ Step 3: Training Incident Prediction Models...")
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# 3a. Logistic Regression
baseline_model = LogisticRegression(random_state=42)
baseline_model.fit(X_train, y_train)
baseline_accuracy = accuracy_score(y_test, baseline_model.predict(X_test))

# 3b. Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_accuracy = accuracy_score(y_test, rf_model.predict(X_test))

# 3c. Gradient Boosting
gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)
gb_model.fit(X_train, y_train)
gb_accuracy = accuracy_score(y_test, gb_model.predict(X_test))

# 3d. Deep Neural Network (Incident Data)
dnn_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])
dnn_model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])
dnn_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=0)
dnn_pred_prob = dnn_model.predict(X_test, verbose=0)
dnn_pred = (dnn_pred_prob > 0.5).astype(int)
dnn_accuracy = accuracy_score(y_test, dnn_pred)


# ==================== 4. URL FEATURE EXTRACTOR & DEDICATED MODEL ====================
print("üîó Step 4: Training Dedicated URL Classifier...")
def extract_url_features(url):
    """Extracts structural features from a URL."""
    features = {}
    features['url_length'] = len(url)
    features['num_digits'] = sum(c.isdigit() for c in url)
    features['num_special_chars'] = len(re.findall(r'[\W_]', url))
    features['has_https'] = int(url.startswith("https"))
    
    # Check for IP in netloc (domain part)
    netloc = urlparse.urlparse(url).netloc
    features['has_ip'] = int(bool(re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', netloc))) 
    
    features['num_subdirs'] = url.count('/')
    features['num_dots'] = url.count('.')
    features['has_at_symbol'] = int('@' in url)
    features['has_hyphen'] = int('-' in url)
    features['domain_length'] = len(netloc)
    return pd.DataFrame([features])

# Generate mock data for URL classifier
urls, y_url_risk = generate_mock_url_data()
X_url_features_raw = pd.concat([extract_url_features(u) for u in urls], ignore_index=True)
url_feature_cols = X_url_features_raw.columns

# Scale URL features
url_scaler = StandardScaler()
X_url_scaled = url_scaler.fit_transform(X_url_features_raw)

# Split URL data
X_url_train, X_url_test, y_url_train, y_url_test = train_test_split(
    X_url_scaled, y_url_risk, test_size=0.2, random_state=42, stratify=y_url_risk
)

# Train dedicated URL Deep Neural Network
url_dnn_model = Sequential([
    Dense(16, activation='relu', input_shape=(X_url_train.shape[1],)),
    Dense(8, activation='relu'),
    Dense(1, activation='sigmoid')
])
url_dnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
url_dnn_model.fit(X_url_train, y_url_train, epochs=15, batch_size=16, verbose=0)

# Evaluate URL Model (for logging)
url_pred_prob = url_dnn_model.predict(X_url_test, verbose=0)
url_accuracy = accuracy_score(y_url_test, (url_pred_prob > 0.5).astype(int))
url_auc = roc_auc_score(y_url_test, url_pred_prob)

print(f"URL Classifier Accuracy: {url_accuracy:.4f}, AUC: {url_auc:.4f}")

# ==================== 5. MODEL COMPARISON & SAVING ====================
print("üíæ Step 5: Saving Models...")
y_pred_proba_baseline = baseline_model.predict_proba(X_test)[:, 1]
y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]
y_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]
dnn_auc = roc_auc_score(y_test, dnn_pred_prob)

comparison_df = pd.DataFrame({
    'Model': ['Logistic Regression', 'Random Forest', 'Gradient Boosting', 'Deep Neural Network (Incident)'],
    'Accuracy': [baseline_accuracy, rf_accuracy, gb_accuracy, dnn_accuracy],
    'AUC Score': [
        roc_auc_score(y_test, y_pred_proba_baseline),
        roc_auc_score(y_test, y_pred_proba_rf),
        roc_auc_score(y_test, y_pred_proba_gb),
        dnn_auc
    ]
})

# Save main models
joblib.dump(baseline_model, 'baseline_model.pkl')
joblib.dump(rf_model, 'random_forest_model.pkl')
joblib.dump(gb_model, 'gradient_boosting_model.pkl')
dnn_model.save('incident_dl_model.h5')
joblib.dump(scaler, 'incident_scaler.pkl')
joblib.dump(le_attack, 'label_encoder_attack.pkl')
joblib.dump(le_industry, 'label_encoder_industry.pkl')
joblib.dump(le_country, 'label_encoder_country.pkl') # Save country encoder

# Save dedicated URL model and scaler
url_dnn_model.save('url_dl_model.h5')
joblib.dump(url_scaler, 'url_scaler.pkl')
joblib.dump(url_feature_cols, 'url_feature_cols.pkl')


# ==================== 6. STREAMLIT DASHBOARD ====================
def run_dashboard():
    st.set_page_config(page_title="Cybersecurity Threat Predictor", layout="wide")
    st.title("üõ°Ô∏è Integrated Cybersecurity Prediction Dashboard")
    st.markdown("---")

    # Load all models and components
    @st.cache_resource
    def load_all_models():
        try:
            # Main Incident Models
            models = {
                'Logistic Regression': joblib.load('baseline_model.pkl'),
                'Random Forest': joblib.load('random_forest_model.pkl'),
                'Gradient Boosting': joblib.load('gradient_boosting_model.pkl'),
                'Deep Neural Network (Incident)': load_model('incident_dl_model.h5')
            }
            incident_scaler = joblib.load('incident_scaler.pkl')
            
            # Label Encoders
            le_attack = joblib.load('label_encoder_attack.pkl')
            le_industry = joblib.load('label_encoder_industry.pkl')
            le_country = joblib.load('label_encoder_country.pkl')
            
            # URL Detection Models
            url_model = load_model('url_dl_model.h5')
            url_scaler = joblib.load('url_scaler.pkl')
            url_feature_cols = joblib.load('url_feature_cols.pkl')
            
            return models, incident_scaler, le_attack, le_industry, le_country, url_model, url_scaler, url_feature_cols
        except Exception as e:
            st.error(f"Error loading models. Please ensure the full script ran successfully and created all required files. Error: {e}")
            return None, None, None, None, None, None, None, None

    models, incident_scaler, le_attack, le_industry, le_country, url_model, url_scaler, url_feature_cols = load_all_models()
    
    if models is None:
        return

    # Create two tabs
    tab1, tab2 = st.tabs(["üìä Incident Risk Analyzer", "üåê URL Threat Checker"])

    # ========== TAB 1: INCIDENT METRIC PREDICTION ==========
    with tab1:
        st.header("Incident Risk Analysis")
        st.subheader("Input Threat Parameters")
        
        # Sidebar inputs
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            financial_loss = st.slider("Financial Loss ($M)", 0.0, 200.0, 50.0, key='fl')
            affected_users = st.slider("Affected Users", 0, 50000, 10000, key='au')
            
        with col2:
            response_time = st.slider("Response Time (hours)", 0.0, 24.0, 5.0, key='rt')
            data_breach_size = st.slider("Data Breach Size (MB)", 0, 5000, 1000, key='dbs')
            
        with col3:
            network_traffic = st.slider("Network Traffic (GB)", 0.0, 2000.0, 800.0, key='nt')
            vulnerability_score = st.slider("Vulnerability Score", 1, 10, 5, key='vs')
        
        with col4:
            attack_type = st.selectbox("Attack Type", le_attack.classes_, key='at')
            industry = st.selectbox("Target Industry", le_industry.classes_, key='ti')
            country = st.selectbox("Country", le_country.classes_, key='ct') # Added country input
            year = st.slider("Year", 2020, 2024, 2023, key='yr')
            month = st.slider("Month", 1, 12, 6, key='mo')
        
        # Prepare input
        loss_per_user = financial_loss / (affected_users + 1)
        
        input_data = pd.DataFrame({
            'financial_loss': [financial_loss],
            'affected_users': [affected_users],
            'response_time': [response_time],
            'data_breach_size': [data_breach_size],
            'network_traffic': [network_traffic],
            'vulnerability_score': [vulnerability_score],
            'loss_per_user': [loss_per_user],
            'attack_type_encoded': [le_attack.transform([attack_type])[0]],
            'industry_encoded': [le_industry.transform([industry])[0]],
            'country_encoded': [le_country.transform([country])[0]], # Transform country
            'year': [year],
            'month': [month]
        })
        
        input_scaled = incident_scaler.transform(input_data)
        
        st.markdown("---")
        st.subheader("Model Predictions")
        
        cols = st.columns(4)
        
        for i, (name, model) in enumerate(models.items()):
            if "Deep Neural Network" in name:
                prob = model.predict(input_scaled, verbose=0)[0, 0]
            else:
                prob = model.predict_proba(input_scaled)[0, 1]
            
            risk = "HIGH RISK" if prob > 0.5 else "LOW RISK"
            
            with cols[i]:
                st.metric(f"Risk Probability ({name})", f"{prob:.2%}")
                st.markdown(f"**Prediction:** {'<span style=\\"color:red; font-weight:bold;\\">'+risk+'</span>' if risk == 'HIGH RISK' else '<span style=\\"color:green; font-weight:bold;\\">'+risk+'</span>'}", unsafe_allow_html=True)

        st.markdown("---")
        st.subheader("Model Performance Summary")
        st.dataframe(comparison_df.style.format({'Accuracy': '{:.3f}', 'AUC Score': '{:.3f}'}), use_container_width=True)

        col1, col2 = st.columns(2)
        with col1:
            fig1 = px.bar(comparison_df, x='Model', y='Accuracy', color='Model', title="Model Accuracy")
            st.plotly_chart(fig1, use_container_width=True)
        with col2:
            fig2 = px.bar(comparison_df, x='Model', y='AUC Score', color='Model', title="AUC Comparison")
            st.plotly_chart(fig2, use_container_width=True)


    # ========== TAB 2: URL THREAT CHECKER (Dedicated DL Model) ==========
    with tab2:
        st.header("üåê Dedicated URL Threat Detection")
        st.markdown("This feature uses a separate Deep Learning model trained specifically on URL structural characteristics to predict risk.")

        user_url = st.text_input("üîó Enter URL to check:", value="http://secure-login.bank-update.com/verify-account", key='url_input')

        if st.button("üö® Check URL Risk"):
            if user_url:
                try:
                    # 1. Extract features using the dedicated function
                    features = extract_url_features(user_url)
                    
                    # 2. Reorder features and scale using the dedicated URL scaler
                    # Align columns to what the URL model was trained on
                    features_aligned = features.reindex(columns=url_feature_cols, fill_value=0)
                    X_url_scaled_input = url_scaler.transform(features_aligned)
                    
                    # 3. Predict using the dedicated URL model
                    prob = url_model.predict(X_url_scaled_input, verbose=0)[0, 0]
                    prediction = 1 if prob > 0.5 else 0

                    st.subheader("Analysis Result")
                    
                    if prediction == 1:
                        st.error(f"üö® **MALICIOUS URL DETECTED** (Risk Probability: {prob:.2%})")
                        st.markdown("**Recommendation:** Do not click. This URL exhibits structural characteristics common to phishing or malware links.")
                    else:
                        st.success(f"‚úÖ **URL Appears SAFE** (Risk Probability: {prob:.2%})")
                        st.markdown("**Recommendation:** This URL's structure appears normal.")
                        
                    st.markdown("---")
                    with st.expander("View Extracted URL Features"):
                        st.dataframe(features)

                except Exception as e:
                    st.error(f"‚ö†Ô∏è An internal error occurred during URL checking. Error: {e}")
            else:
                st.warning("Please enter a URL to check.")

# ==================== 7. RUN APP ====================
if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("üöÄ CYBERSECURITY ML PIPELINE - FULL EXECUTION")
    print("This trains and saves two ML pipelines: Incident Risk & URL Threat.")
    print("=" * 60)
    
    # Run the dashboard
    run_dashboard()
